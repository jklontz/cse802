\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage[lofdepth,lotdepth]{subfig}
\usepackage{float}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tikz}
\usepackage{caption}
\usetikzlibrary{arrows,shapes,trees} % loads some tikz extensions
 
\begin{document}
\graphicspath{{images/}}
 
\title{CSE 802 - Final Project Report\\ ImageNet Classification}
\author{Sunpreet Arora \& Josh Klontz\\
}
 
\maketitle

\section{Introduction}
Automated image classification is a classic problem in the domain of computer vision and pattern recognition.
Although numerous algorithms exist for categorizing images based on their distinct features, large scale image classification is still considered an open problem.
This is primarily because as the number of classes increases, it becomes increasingly difficult to find a distinctive set of features to distinguish between them.
Compounding the problem, the computational complexity of the classification task generally scales with the number of classes.

Arguably the most common approach to image classification is the popular information retrieval model known as the bag-of-words (BOW) model.
A \textit{visual vocabulary} is first created based on features computed from the training set.
Images are then represented and classified based on the frequency of occurrence of their features (also referred to as \textit{visual words}) in the visual vocabulary.

Several databases have been collected to advance the state-of-the-art in image classification including Caltech 101 \cite{caltech101}, and PASCAL VOC \cite{pascal09}.
ImageNet \cite{imagenet} is one of the most recent large scale image databases with 10,000 image classes, and over a million images.
Convolutional neural networks (CNNs) have been reported to give the best rank-5 accuracy of around 85\% on the ImageNet database \cite{alex2012}.
However, they were implemented on massively parallel GPU's.
Another effective approach with a reported top-5 accuracy of 74\% involves Fisher vectors \cite{csurka2011fisher}.

The scope of this project is limited to classifying a subset of 20 classes (the 10 hardest and 10 easiest) from the ImageNet database.
We are given about 20000 images for training, 1000 images for validation and 5000 images for testing.
The objective is to come up with a classification framework that maximizes rank-5 accuracy.

\section{Challenges}
\label{sec:challenges}
Despite working on a 20 class subset of ImageNet, the classification problem is still very challenging.
Some of the most challenging aspects are discussed below:

\begin{itemize}
\item \textit{Intraclass Variability}:
Since ImageNet database is organized in a semantic hierarchy, exemplars from the same image class exhibit a considerable amount of variation or \textit{intraclass variability} amongst themselves.
For example, both a gong and a wind chime belong to the semantic category (Figure \ref{fig1:intraclassvariability}).

\item \textit{Interclass Similarity}:
Similarly, exemplars from different classes such as spatula and ladle look visually similar to each other, or in other words exhibit \textit{interclass similarity} (Figure \ref{fig1:interclasssimilarity}).

\item \textit{Scale}:
The object of interest is present at different scales in different exemplar images for two canoe images (Figure \ref{fig1:scale}).

\item \textit{Occlusion}: Sometimes the object of interest is occluded by another object in the image, thereby reducing the amount of useful information content for the classification task like the drill and the spatula (Figure \ref{fig1:occlusion}).

\item \textit{Pose}:
Variations in pose of the object of interest are also observed amongst different exemplar images. The hatchets in this example face different directions (Figure \ref{fig1:pose}).

\item \textit{Illumination}:
The database contains images acquired from a lot of different sources with differing illumination conditions, like the cleavers shown below (Figure \ref{fig1:illumination}).

\item \textit{Clutter}: There is a lot of background clutter in some of the exemplar images, or in other words there is more useless or distracting information present in the image than the useful signal for the purpose of image classification (Figure \ref{fig1:clutter}).
The correct classes in this example are cleaver and spatula.

\end{itemize}

\begin{figure}
\centering
\subfloat[intraclass variability]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{chime1}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{chime2}
\label{fig1:intraclassvariability}
}
\subfloat[interclass similarity]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{spatula1}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{ladle1}
\label{fig1:interclasssimilarity}
}
\qquad
\subfloat[scale]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{canoe1}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{canoe2}
\label{fig1:scale}
}
\subfloat[occlusion]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{pdrill1}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{spatula2}
\label{fig1:occlusion}
}
\qquad
\subfloat[pose]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{hatchet1}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{hatchet2}
\label{fig1:pose}
}
\subfloat[illumination]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{cleaver2}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{cleaver3}
\label{fig1:illumination}
}
\qquad
\subfloat[clutter]{
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{cleaver1}
\includegraphics[width = 0.2\textwidth, height = 0.15\textheight]{spatula3}
\label{fig1:clutter}
}
\caption{Some challenges in image classification}
\end{figure}

\begin{figure}
\includegraphics[width = 1\textwidth]{flowchart}
\caption{The proposed classification framework}
\end{figure}

\section{Classification Framework}
For the given classification task, we propose a three stage framework based on the bag-of-words (BOW) model.
The first step comprises of densely sampling three different types of features at multiple scales and generating a \textit{visual vocabulary} for each feature type.
K-means clustering is then applied to create a \textit{visual codebook} for each distinct feature type.
Finally, radial basis function (RBF) support vector machines (SVM) are trained on each codebook, and their prediction outputs are combined to generate the class labels.

\subsection{Feature Descriptors}
Recall that the fundamental objective is to represent images in terms of features which can sufficiently distinguish between different classes for the purpose of image classification.
Given the challenges discussed in section \ref{sec:challenges}, it seems likely that no single feature descriptor can adaquately distinguish between all of the classes.
Therefore, in our approach we use three distinct descriptors that capture different information in the image:

\begin{itemize}
\item \textit{Scale Invariant Feature Transform (SIFT)}:
SIFT \cite{lowe04} is a 128 dimensional descriptor comprised of 16 8-bin weighted and normalized histograms of oriented gradients.
Figure \ref{fig:SIFT} illustrates the layout of the descriptor.
SIFT has been used successfully in a wide variety of image retrieval tasks since its introduction over a decade ago.

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{SIFT}
\caption{Visualization of the SIFT descriptor.}
\label{fig:SIFT}
\end{figure}

\item \textit{Local Binary Patterns (LBP)}:
LBP \cite{ahonen06} is a 59 dimensional histogram where each bin represents a distinct local pattern.
LBP is first computed by thresholding adjacent pixels to generate an 8-bit pattern as shown in figure \ref{fig:lbp}.
Figure \ref{fig:lbpu2} illustrates the next step which is to discard all but the unform patterns and reduce the possible number of patterns from 256 to 59.
As a final step, a historgram is computed over a local region to generate the descriptor.
This representation has proven especially popular in the face recognition community.

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{LBPSimple}
\caption{Every value is compared against the center to generate an 8-bit local binary pattern.}
\label{fig:lbp}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{LBPU2}
\caption{
The first row shows some of the possible 58 uniform LBP patterns with zero or two transitions between a 0 and a 1 bit.
The bottom row shows some of the 198 possible non-uniform patters that all get mapped to the 59th $LBP^{U2}$ pattern.
}
\label{fig:lbpu2}
\end{figure}

\item \textit{Color}:
Visual inspection of the training images suggests color can also be used for identifying some classes.
In this case we chose to use the \emph{RG} color space described in \cite{sande10} due to it's accuracy in image retrieval tasks.
Equation \ref{eq:rg} shows how this normalized color space is computed.
\begin{equation}
\begin{split}
R &= \frac{r}{r+g+b} \\
G &= \frac{g}{r+g+b} \\
\end{split}
\label{eq:rg}
\end{equation}
In our implementation we chose to quantized $R$ and $G$ into 32-bin histograms to generate a 64 dimensional color desciptor.
\end{itemize}

\begin{figure}
\centering
\subfloat[Dense feature extraction]{
\includegraphics[width = 0.4\textwidth, height = 0.25\textheight]{dog_intdet}
}
\subfloat[Visual codebook generation]{
\includegraphics[width = 0.4\textwidth, height = 0.25\textheight]{dog_quantized}
}
\caption{
Illustration of the dense feature extraction and the codebook generation process.
Features are first extracted at over a dense grid at several resolutions.
They are then clustered and represented in a 1024-word codebook.}
\label{fig:codebook}
\end{figure}

\subsection{Multi-scale dense feature extraction}
The object of interest in an image can occur at a varying scale, therefore we extract descriptors at three resolutions in an attept to remain invariant scale.
In particular, each descriptor is extracted in a 15x15, 10x10, and 5x5 grid.
Since all of the descriptors are histograms, they are normalized to unit $L_1$ norm so that descriptors extracted from different resolutions can be compared and clustered in the same feature space.

\subsection{Visual codebook generation}
For each dense descriptor type, we run k-means three times and pick the cluster centers from the run with the lowest entropy.
Experimentation with different values of $k$ suggested that $k=1024$ was a good choice for our problem.
Figure \ref{fig:codebook} illustrates the dense feature extraction and codebook generation steps. 

\subsection{Classification}
The final step in the proposed paradigm involves training classifiers based on the generated visual codebooks, and then using them to predict test samples.
There are many considerations within this step including the choice of classifiers, parameter tuning, and the combination of classifiers.

\subsubsection{Choice of Classifiers}
We initially experimented with using the nearest neighbor classifier as well as using boosted decision stumps.
The rank-5 accuracies using the SIFT codebook (5-fold cross validation on the training data) were around 66\% and 78\% respectively for these classifiers.
Radial basis Support Vector Machines (RBF-SVMs) outperformed all other classifiers in our initial set of experiments and we therefore opted to use them as the base classifier for each of three different visual codebooks.

\subsubsection{Parameter Selection}
Effectiveness of SVM classifiers depends on the selection of the right set of parameters.
There are two different parameters for RBF-SVM namely the soft margin parameter $C$ and the kernel width $G$.
To obtain the optimum set of parameters, we do a coarse-to-fine grid search over the 2-dimensional space of $C$ and $G$ as shown in Figure \ref{fig:gridsearch}.
This involves optimizing the 5-fold cross validation accuracies on the training set over a grid of coarse combinations of parameters $C$'s and $G$'s, and then fine tuning the parameters by doing a finer search inside the coarser grid.

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth, height =0.2\textheight]{gridsearch}
\caption{Coarse-to-fine grid search for RBF-SVM optimum parameters}
\label{fig:gridsearch}
\end{figure}

\subsubsection{Classifier Combination}
Since the three non-linear SVM classifiers are trained on the three different visual codebooks corresponding to multiscale SIFT, LBP and Color features, they work on features which capture different types of information content from an image.
Therefore, assuming that the classifiers are independent is a reasonable assumption in this scenario.
Motivated by the notion of independence, fuse the classifiers using the product fusion rule, as shown in Figure \ref{fig:classifiercombination}.
Each classifier independently outputs the probability of the test sample belonging to each of the different classes.
The probability values corresponding to each of the classes are multiplied together to yield the final probability estimate.
These estimates are then sorted in a descending order to get the top-5 class labels for the test sample.

\begin{figure}
\includegraphics[width = 1\textwidth]{classifiercombination}
\caption{Classifier Combination using Product Fusion}
\label{fig:classifiercombination}
\end{figure}

\section{Results}
The following statistics show the accuracy of the method described above. Overall we were able to achieve 89\% rank-5 accuracy on the problem.

\begin{itemize}
\item Top-5 accuracy:
\begin{itemize}
\item \textbf{Training Set}: 99.99\%
\item \textbf{Validation Set}: 88.55\%
\item \textbf{Test Set}: 88.94\%
\end{itemize}
\item Classes where the approach really works well: \textit{odometer, rapeseed, website}.
\item Classes where the approach does not work very well: \textit{lopener, hatchet, cleaver}.
\end{itemize}

\begin{center}
\begin{tabular}[]{| c | c | c | }
\hline
\multicolumn{2}{|c|}{\textbf{Classification Accuracy} (\%)} \\
\hline
SIFT & 81.55\\
\hline
LBP & 82.6\\
\hline
Color & 77.58\\
\hline
SIFT + LBP & 84.6\\
\hline
SIFT + LBP + Color & \textbf{88.55}\\
\hline
\end{tabular}
\captionof{table}{Classification accuracy of the proposed approach on the validation set.}
\end{center}

\bibliographystyle{plain}
\bibliography{report}

\end{document}